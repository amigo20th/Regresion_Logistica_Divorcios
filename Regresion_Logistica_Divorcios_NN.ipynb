{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regresion_Logistica_Divorcios_NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru6jbXEwF1Dh",
        "colab_type": "text"
      },
      "source": [
        "<h1>Regresión Logística para la predicción de divorcios</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gObF_TG5GARn",
        "colab_type": "text"
      },
      "source": [
        "En este Notebook vamos a hacer una regresión logístia por medio de una sola neurona, o perceptró. El proceso va a ser llevado desde un inicio con el mayor nivel de explicación que me sea posible dar.\n",
        "\n",
        "El ejercicio está basado en el Notebook de Andrew Ng llamado Logistic Regression with a Neural Network, aunque el ejercicio está excelentemente explicado quiero replicarlo, esto lo voy a hacer en mis propias palabras es por eso que lo estoy haciendo en español.\n",
        "\n",
        "Llamamos nuestras librerias a ocupar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afWmtlmIF-LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDTEw7qlWJFp",
        "colab_type": "text"
      },
      "source": [
        "Cargamos el conjunto de datos necesarios para la realización de la regresión losgística.\n",
        "\n",
        "En este caso el conjunto de datos está tomado del siguiente link:\n",
        "[Conjunto de datos sobre cuestionario de divorcios](https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set)\n",
        "\n",
        "Para mayor información sobre el conjunto de datos checar la publicación.\n",
        "\n",
        "*Yöntem, M , Adem, K , İlhan, T , Kılıçarslan, S. (2019). DIVORCE PREDICTION USING CORRELATION BASED FEATURE SELECTION AND ARTIFICIAL NEURAL NETWORKS. Nevşehir Hacı Bektaş Veli University SBE Dergisi, 9 (1), 259-273.* [Publicación](https://dergipark.org.tr/tr/pub/nevsosbilen/issue/46568/549416)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdnKpTw7IWzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/sample_data/divorce.csv\", sep = '\\t', header=None)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qO1ytZ0XKIg",
        "colab_type": "text"
      },
      "source": [
        "Ya una vez cargados los datos en un DataFrame de pandas, vamos a ver la distribución de las dos clases (divorciados|casados) para validar que no estéé desbalanceado el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wE5TScXXiVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9adaf886-1b15-4ab3-f8f1-b303514a4b66"
      },
      "source": [
        "print(Counter(df.iloc[:, -1]))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0: 86, 1: 84})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MyXwVmSX3AX",
        "colab_type": "text"
      },
      "source": [
        "Se puede observar que tiene la cantidad de datos de las siguientes clases:\n",
        "\n",
        "| Divorciados  | Casados     |\n",
        "|:------------:|:-----------:|\n",
        "|86            |84           |\n",
        "\n",
        "Por lo tanto se puede observar que efectivamente estamos ante un conjunto de datos balanceado.\n",
        "\n",
        "Al momento de dividir el conjunto de datos en el conjunto de entrenamiento *(train)* y el conjunto de prueba *(test)* debemos asegurarnos de que ese balanceo sea consistente así que vamos a separar el conjunto de datos en divorciados y casados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f0u_CqJ2tIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataframe de divorciados\n",
        "df_div = df[df[54] == 1]\n",
        "# dataframe de los casados\n",
        "df_cas = df[df[54] == 0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNcV0l7EZYbW",
        "colab_type": "text"
      },
      "source": [
        "Solo vamos a crear temporalmente las variables *target* que en este caso contienen al atributo clase. Son temporales porque train_test_split lo requiere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p58dNMeb4L95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_div = df_div.iloc[:, -1]\n",
        "target_cas = df_cas.iloc[:, -1]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HjxPO2MlwuO",
        "colab_type": "text"
      },
      "source": [
        "Validamos que las dimensiones sigan estando correctas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmiKV7-164g8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fc8b0ef1-e6b0-4208-bd05-615f3c8e3ec6"
      },
      "source": [
        "print(df_div.shape)\n",
        "print(df_cas.shape)\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(84, 55)\n",
            "(86, 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veZku2L1Zyt3",
        "colab_type": "text"
      },
      "source": [
        "Ahora si pasamos a dividir en conjunto de entranamiento y conjunto de prueba a los dos *DataFrame* que teniamos, cabe recordar que los conjuntos, *y_train* y *y_test* son temporales y los creamos para poder ejecutar *train_test_split*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spzf0EL64uxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_div, X_test_div, y_train_div, y_test_div = train_test_split(df_div, target_div, train_size=0.8)\n",
        "X_train_cas, X_test_cas, y_train_cas, y_test_cas = train_test_split(df_cas, target_cas, train_size=0.8)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-l2qm9uaOBM",
        "colab_type": "text"
      },
      "source": [
        "Validamos que los conjuntos de entrenamiento y prueba tengan las dimensiones correctas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbQO5tG_42De",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "1bd2b2e5-1ad7-45e5-a442-9f18b4f517b7"
      },
      "source": [
        "print(X_train_div.shape)\n",
        "print(X_test_div.shape)\n",
        "print()\n",
        "print(X_train_cas.shape)\n",
        "print(X_test_cas.shape)\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(67, 55)\n",
            "(17, 55)\n",
            "\n",
            "(68, 55)\n",
            "(18, 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc30u8XMaVbI",
        "colab_type": "text"
      },
      "source": [
        "Efectivamente, están conrrectas ya que el 20 % de 84 es aproximadamente 17 (16.8) y el correspondiente 20 % de los casados que son 86 es cerca de 18 (17.2).\n",
        "\n",
        "Todo sumado son 170 datos, los cuales son con los que iniciamos.\n",
        "\n",
        "Ahora, concatenamos los conjuntos de entrenamiento (divorciados y casados) y los de prueba, así no quedará un solo solo conjunto de datos para cada uno de ellos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YZkdkSt5kb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pd.concat((X_train_div, X_train_cas), axis=0)\n",
        "X_test =  pd.concat((X_test_div , X_test_cas), axis=0)\n",
        "y_train = pd.concat((y_train_div, y_train_cas), axis=0)\n",
        "y_test =  pd.concat((y_test_div,  y_test_cas), axis=0)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zotVMEq8bLYY",
        "colab_type": "text"
      },
      "source": [
        "Validamos que las dimensiones sigan correctas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkHixjkS6Znj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "75407484-b7e3-47d2-d361-96a76fa745c2"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(135, 55)\n",
            "(35, 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0ztqOUWbO6d",
        "colab_type": "text"
      },
      "source": [
        "Ahora, vamos a intercambiar filas de los dos conjuntos de datos de una manera aleatoria, ya que en el último paso de concatenación dejamos a los divorciados todos juntos y a los casados todos juntos, esto se hace para evitar que el perceptrón no detecte el patrón de las clases en lugar de depenter puramente de sus entradas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crw8DAYdQVZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.sample(frac=1.0, random_state=1)\n",
        "X_test = X_test.sample(frac=1.0, random_state=1)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCpG_OkWbuTG",
        "colab_type": "text"
      },
      "source": [
        "Lo que vamos a hacer ahora es crear los verdaderos *y_train* y *y_test* que son la variables que contienen sus respectivas clases a las que perteneces dichos datos.\n",
        "\n",
        "De igual manera le quitamos el conjunto de clases a los conjuntos de datos de entrenamiento y prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv78tQxkREJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = X_train.iloc[:, -1]\n",
        "y_test = X_test.iloc[:, -1]\n",
        "X_train = X_train.iloc[:, :-1]\n",
        "X_test = X_test.iloc[:, :-1]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrMqetwPcNB5",
        "colab_type": "text"
      },
      "source": [
        "Validamos que los índices de las clases pertenezcan a sus respectivos conjuntos de datos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fjX4SvQNgon",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "63d6255c-c44f-4d05-ce1a-ca751a2fab67"
      },
      "source": [
        "print(X_train.index)\n",
        "print(y_train.index)\n",
        "print(X_test.index)\n",
        "print(y_test.index)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([ 12,   0, 109,  62,  75, 119,   1,  83,  61, 165,\n",
            "            ...\n",
            "             79, 144,  93,   6,  59, 162, 148,  97, 106,  42],\n",
            "           dtype='int64', length=135)\n",
            "Int64Index([ 12,   0, 109,  62,  75, 119,   1,  83,  61, 165,\n",
            "            ...\n",
            "             79, 144,  93,   6,  59, 162, 148,  97, 106,  42],\n",
            "           dtype='int64', length=135)\n",
            "Int64Index([ 29, 139,  49, 102, 125, 169, 111, 155, 134, 152, 108,  88,   7,\n",
            "              8, 116, 120,  30, 157,  87, 113, 145, 131,  43,   9,  44, 161,\n",
            "             22,  47,  38,  78,   2,   5,  32,  28,  80],\n",
            "           dtype='int64')\n",
            "Int64Index([ 29, 139,  49, 102, 125, 169, 111, 155, 134, 152, 108,  88,   7,\n",
            "              8, 116, 120,  30, 157,  87, 113, 145, 131,  43,   9,  44, 161,\n",
            "             22,  47,  38,  78,   2,   5,  32,  28,  80],\n",
            "           dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3RuV0o5cZ5w",
        "colab_type": "text"
      },
      "source": [
        "Ahora validamos que las dimensiones finales estén correctas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFq0J1cLNm21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "45a3db4a-ad3a-4b2c-804f-6def567a9490"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(135, 54)\n",
            "(135,)\n",
            "(35, 54)\n",
            "(35,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55y_eh1UcgWE",
        "colab_type": "text"
      },
      "source": [
        "Ahora terminamos con un conjunto de entremiento con 135 datos y 54 atributos, con su respectiva lista de clases supervisadas a las que perteneces esos datos; también tenemos 35 datos de prueba con sus mismos 54 atributos y su varible de clases.\n",
        "\n",
        "|conjunto de datos|dimensiones|\n",
        "|---|:-:|\n",
        "|X_train|(135,54)|\n",
        "|y_train|(135, )|\n",
        "|X_test|(35,54)|\n",
        "|y_test|(35, )|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW3VPV6Edfmj",
        "colab_type": "text"
      },
      "source": [
        "Ahora, apartir de aquí vamos a empezar a trabajar en *numpy* ya que vamos a centrarnos en hacer operaciones más que tratamiento de datos. Oficialmente terminó nuestra etapa de preparación de datos y análisis exploratorio de datos.\n",
        "\n",
        "Comenzamos con el modelado.\n",
        "\n",
        "Vamos a sacar el objeto *numpy* de *pandas*. También vamos a preparar los datos de ambos conjuntos de X, esto ya que cada dato debe estar en una columna y cada atributo debe de estar en cada fila, esto es necesario para poder aplicar la vectorización en *numpy* y poder aplicar la ecuación lineal vectorizada de una menera óptima.\n",
        "\n",
        "La ecuación de la recta vectorizada es la siguiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrpEwkxcflGf",
        "colab_type": "text"
      },
      "source": [
        "$$z = W^{T}X + b$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV0IwnUDkSlK",
        "colab_type": "text"
      },
      "source": [
        "Cabe mencionar que en este ejemplo el vector de los pesos originalmente es un vector columna con dimensiones (número atributos, 1) por lo cual al tenerlo en su traspuesta nos queda en dimensiones (1, número atributos), esto permite una multiplicación matricial con X ya que sus dimensiones son correctas, es decir\n",
        "\n",
        "dim(W.T) = (1, número atributos)\n",
        "\n",
        "dim(X) = (número atributos, m)\n",
        "\n",
        "Donde número de atributos es 54 y m = 135 para el conjunto de entrenamiento.\n",
        "\n",
        "b en este caso es un escalar ya que solo tenemos un perceptrón."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSqx0p_-6si3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sacamos el numpy de cada dataframe y lo guardamos con su \n",
        "# traspuesta para acomodarlo en el convenio\n",
        "X_train = X_train.values\n",
        "X_train = X_train.T\n",
        "\n",
        "X_test = X_test.values\n",
        "X_test = X_test.T"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8O7h_K2e0Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizamos el conjunto X de entrenamiento y de test\n",
        "# como el valor máximo que podemos tener es 4 solo vamos a dividir entre en máximo\n",
        "X_train = X_train / 4\n",
        "X_test = X_test / 4\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BynDo6SJlNQc",
        "colab_type": "text"
      },
      "source": [
        "Validamos que las dimensiones sean las mencionadas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23bQN-f6GWAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f63cb5a6-e325-41e5-be01-2e6608a49bae"
      },
      "source": [
        "# Como podemos ver el valor de m en nuestro ejemplo es 135\n",
        "# y el tamaño del conjunto de prueba es de 35\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54, 135)\n",
            "(54, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qUkMJYylTsZ",
        "colab_type": "text"
      },
      "source": [
        "Ahora vamos a preparar los conjuntos y ya que estas deben de estar en un vector fila y actualmente están en un conjunto columna.\n",
        "\n",
        "OJO, En este punto son adimensionales, lo cual nos puede crear problemas al momento de hacer cálculos con ellas, por lo tanto primero es ponerlas en el plano dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TXc-kAvZwC0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dfd4a035-583d-405d-e5a1-4a14be576101"
      },
      "source": [
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(135,)\n",
            "(35,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39V6WZZqaMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train.reshape((1, y_train.shape[0]))\n",
        "y_test = y_test.reshape((1, y_test.shape[0]))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xEGnrylbHb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5240d33d-5bf5-4ae0-d1db-565b4d0e8aeb"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 135)\n",
            "(1, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp-rLUQTJpiP",
        "colab_type": "text"
      },
      "source": [
        "Ahora sí, pasamos a crear las funciones correspondientes para poder ejecutar una regreción logística con una neurona por medio de *backpropagation*.\n",
        "\n",
        "Como la activación de dicha neurona va a ser con una función sigmoide, la declaramos.\n",
        "\n",
        "OJO, las funciones están en inglés ya que son copiadas directamente de mi propio ejercicio hecho en el notebook de Andrew Ng."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoSTt1AYHjra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Función de activación\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Computa el sigmoide de z\n",
        "\n",
        "    Argumentos:\n",
        "    z -- Un escalar o un array de numpy de cualquier tamaño.\n",
        "\n",
        "    Returna:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    return s"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76rqFaYdmkHT",
        "colab_type": "text"
      },
      "source": [
        "Nuestros pesos en un principio van a estar inicializados en ceros junto con nuestro sesgo, no es la mejor inicialización y menos con una activacióón sigmoidal ya que da oportunidad a la saturación de neuronas. Así va el ejercicio y en proyectos posteriores vamos a crear más activaciones, en este caso no hay mayor problema ya que es solo una neurona."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok66geGAJ_ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Función de inicialización de pesos en ceros\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    Esta función crea un vector de ceros con dimensiones (dim, 1) para w e inicializa b a cero\n",
        "    \n",
        "    Argument:\n",
        "    dim -- tamaño de el vector w que queremos (o el número de parámetros en este caso)\n",
        "    size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returna:\n",
        "    w -- el vector inicializado de dimensiones (dim, 1)\n",
        "    b -- el escalar inicializado de dimensiones (dim, 1)\n",
        "    \"\"\"\n",
        "    w = np.zeros(shape=(dim, 1))\n",
        "    b = 0\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    return w, b"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGr02vj3m83y",
        "colab_type": "text"
      },
      "source": [
        "## Propagación hacia adelante.\n",
        "Ahora, vamos a crear la propagación hacia adelante de cada uno de los datos, ponderado con sus pesos y sesgo y activado por la sigmoide. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuUURNW4nQme",
        "colab_type": "text"
      },
      "source": [
        "Aquí vamos a explicar qué está pasando detrás del código y a explicar la procedencia de las diferentes variables.\n",
        "\n",
        "A, es el conjunto de activaciones y está definido de la siguiente forma:\n",
        "$$A = \\left\\{ a_{1}, a_{2}, a_{3}, ..., a_{m} \\right\\}$$\n",
        "cada uno de los valores $$a_{i}$$ representan la activación para cada uno de los datos de X, por ejemplo:\n",
        "$$a_i = sigmoid(w_1x_{0,i}+w_2x_{1,i}+w_3x_{2,i}+...+b)$$\n",
        "\n",
        "Antes de ver el costo vamos a ver la ecuación para el error logarítmico, su fórmula es:\n",
        "$$L(a^i,y^i)= -y^i \\ln{(a^i)} - (1-y^i)\\ln{(1-a^i)}$$\n",
        "Esta calcula el error para cada una de las activaciones respecto a su etiqueta en el conjunto Y, su calculo lo hace de la siguiente manera:\n",
        "\n",
        "Cuando y=0 el primer elemento de la ecuación se va a cero y toma el valor de $$L(a^i,0)=-\\ln{(1-a^i)}$$ En este caso si nuestra predicción es cerca de 0 (no exactamente 0 por la salida del sigmoide) el logaritmo natural toma un valor muy pequeño. Por el contrario si nuestra predicción es cercano a 1, el logaritmo natural toma un valor alto.\n",
        "\n",
        "Cuando y=1, el primer elemento de la ecuación toma el control y la segunda se va a 0.\n",
        "$$L(a^i,1)= -\\ln{(a^i)}$$\n",
        "En este caso si nuestra predicción tiene una activación cercana a 0 el logaritmo natural toma un valor muy alto y al contrario si nuestra activación es cercano a 0, el logaritmo natural toma un valor muy pequeño.\n",
        "\n",
        "Ya teniendo el conocimiento de la ecuación para el error logarítmico, vamos a ver la función costo, esta solamente es el promedio de todas las salidas de la función error para cada uno de nuestras activaciones, es decir:\n",
        "$$J(w, b) = \\frac{1}{m}\\sum_{i = 1}^{m}L(a^i, y^i)$$\n",
        "\n",
        "Con eso termina la propagación hacia adelante, con esto ya todos los m ejemplos están evaluados y tenemos la función costo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix8x7pmsSNFI",
        "colab_type": "text"
      },
      "source": [
        "## Propagación hacia atrás (backpropagation).\n",
        "\n",
        "Ahora, lo que nos compete son las siguientes ecuaciones ya que son las responsables de determinar la porporción de descenso que tiene que hacer cada parámetro, en este caso son los pesos y el sesgo. Las ecuaciones son las siguientes:\n",
        "$$w^i := w^i + \\alpha \\frac{\\partial J(w, b)}{\\partial w}$$\n",
        "\n",
        "$$b :=b + \\alpha \\frac{\\partial J(w, b)}{\\partial b}$$\n",
        "\n",
        "Donde *alpha* es la tasa de aprendizaje.\n",
        "\n",
        "Para encontrar las derivadas parciales hay que utilizar la regla de cadena de cálculo, pero primero hay que determinar el orden de la composición de las funciones, quedando de la siguiente manera\n",
        "Primero empezamos teniendo los pesos, el sesgo y el conjunto X para el entrenamiento.\n",
        "$$W, X, b$$\n",
        "Después aplicamos la ecuación lineal, que nos da Z.\n",
        "$$Z = W^TX+b$$\n",
        "Posteriormente aplicamos la función de activación sigmoide a Z y nos devuelve A.\n",
        "$$A = \\sigma(Z)$$\n",
        "Donde sigma es la función sigmoide.\n",
        "Por último calculamos el error logarítmico y nos devuelve L(ai, yi), ya la función costo sería la suma de todas las salidas de L(ai, yi).\n",
        "$$L(a^i,y^i)= -y^i \\ln{(a^i)} - (1-y^i)\\ln{(1-a^i)}$$\n",
        "\n",
        "Por lo tanto, tomando las derivadas parciales de abajo hacia arriba para crear la regla de cadena nos queda.\n",
        "OJO, a pesar de que queremos encontrar J(w, b) vamos a calcular la regla de cadena a partir de L(ai, yi).\n",
        "\n",
        "Entonces la regla de la cadena nos deja las siguientes derivadas parciales para calcular el cambio de los pesos y del sesgo.\n",
        "$$\\frac {\\partial L}{\\partial w} = \\frac {\\partial L}{\\partial a} \\frac {\\partial a}{\\partial z} \\frac {\\partial z}{\\partial w}$$\n",
        "\n",
        "$$\\frac {\\partial L}{\\partial b} = \\frac {\\partial L}{\\partial a} \\frac {\\partial a}{\\partial z} \\frac {\\partial z}{\\partial b}$$\n",
        "\n",
        "Ahora pasamos a desarrollar cada una de las parciales necesarias para el cálculo. Primero calculamos la derivada de L respecto a a.\n",
        "$$\\frac {\\partial L}{\\partial a} = -y (\\frac{1}{a})  - (1-y)\\frac{1}{1-a}(-1) = \\frac{-y}{a}+\\frac{1-y}{1-a}$$\n",
        "\n",
        "\n",
        "$$\\frac{\\partial a}{\\partial z} = -(1+e^{-z})^{-2}(-e^{-z}) = \\frac{e^{-z}+1-1}{(1+e^{-z})^{2}} = \\frac{1+e^{-z}}{(1+e^{-z})^{2}}-\\frac{1}{(1+e^{-z})^{2}} = \\frac{1}{1+e^{-z}} -a^2 = a-a^2 = a(1-a)$$\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial w} = x$$\n",
        "$$\\frac{\\partial z}{\\partial b} = 1$$\n",
        "\n",
        "Por lo tanto la multiplicación de la regla de cadena queda.\n",
        "$$\\frac {\\partial L}{\\partial w} = (\\frac{-y}{a}+\\frac{1-y}{1-a}) (a(1-a))(x) = [-y(1-a)+a(1-y)]x = x(-y+ay+a-ay) = x(a-y)$$\n",
        "\n",
        "$$\\frac {\\partial L}{\\partial b} = a-y$$\n",
        "\n",
        "Por lo tanto, haciendo la suma total para sacar el promedio, quedan las derivadas parciales de la siguiente manera:\n",
        "\n",
        "$$\\frac{\\partial J(w, b)}{\\partial w} = \\frac{1}{m} X(A-Y)^T$$\n",
        "\n",
        "$$\\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i = 1}^{m} (a^i-y^i)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1827-iyLIAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Función de propagación\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implementa la función costo y su gradiente para la propagación explicada anteriormente\n",
        "\n",
        "    Argumentos:\n",
        "    w -- weights o pesos, un array numpy de tamaño (num_atributos[54 en este caso], 1) \n",
        "    b -- bias o sesgo, un escalar\n",
        "    X -- datos con el tamaño (num_atributos[54 en este caso], numbero de ejemplos)\n",
        "    Y -- vector \"etiqueta\" verdad(contiene 0 si esta casado, 1 si está divorsiado) de tamaño (1, numero de ejemplos)\n",
        "\n",
        "    Returna:\n",
        "    cost -- la probabilidad del costo logarítmico negativo para la regresión logística\n",
        "    dw -- gradiente de la perdida con respecto a w, así tiene la misma dimensión de w \n",
        "    db -- gradiente de la perdida con respecto a b, así tiene la misma dimensión de b    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # PROPAGACIÓN HACIA DELANTE (DESDE X A COST)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)                         # computa la activación\n",
        "    cost = np.sum( Y*np.log(A) + (1-Y)*np.log(1-A) ) / -m   # computa el cost\n",
        "    \n",
        "    # PROPAGACIÓN HACIA ATRÁS (PARA ENCONTRAR EL GRADIENTE)\n",
        "    dw = np.dot(X, (A-Y).T) / m\n",
        "    db = np.sum(A-Y) / m\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvzX-lCeeOjA",
        "colab_type": "text"
      },
      "source": [
        "La función optimizar solo va a calcular las derivadas parciales de la función costo y aplicara el gradiente descendiente:\n",
        "$$w := w + \\alpha \\frac{\\partial J(w, b)}{\\partial w}$$\n",
        "\n",
        "$$b :=b + \\alpha \\frac{\\partial J(w, b)}{\\partial b}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st43ZeovRb5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCIÓN: optimizar\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    Esta función optimiza w y b por correr un algoritmo de gradiente descendiente\n",
        "    \n",
        "    Argumentos:\n",
        "    w -- weights o pesos, un array numpy de tamaño (número de atributos, 1)\n",
        "    b -- bias o sesgo, un escalar\n",
        "    X -- datos con el tamaño (num_atributos[54 en este caso], numbero de ejemplos)\n",
        "    Y -- vector \"etiqueta\" verdad(contiene 0 si esta casado, 1 si está divorsiado) de tamaño (1, numero de ejemplos)\n",
        "    num_iterations -- número de iteraciones del ciclo de optimización\n",
        "    learning_rate -- tasa de aprendizaje de la regla de actualización del gradiente descendiente\n",
        "    print_cost -- Si es True imprime la pérdida cada 100 pasos \n",
        "    \n",
        "    Returna:\n",
        "    params -- diccionario que contiene los pesos w y el sesgo b\n",
        "    grads -- diccionario que contiene los gradiente de los pesos y el sesgo respecto a la función cost\n",
        "    costs -- lista de todos los costos computados durante la optimización, esto será usado para graficar la curva de aprendizaje     \n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "        #  Resupera las derivadas desde grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        w = w - learning_rate*dw\n",
        "        b = b - learning_rate*db\n",
        "        \n",
        "        # Guarda los costos\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Imprime los costos cada 100 iteraciones de entrenamiento\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQqIiFI5enh0",
        "colab_type": "text"
      },
      "source": [
        "La función predecir es la encargada de otorgar solo ceros y unos a nuestras predicciones, ya que hasta ahora solo las teniamos en activaciones que no llegaban a ser cero o uno como tal, si una activación es menor que o igual que 0.5 entonces la predicción es 0 y por el contrario si nuestra activación es mayor a 0.5 entonces nuestra predicción es 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_bC_WfuReG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCIÓN: predecir\n",
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predice ya sea si la etiqueta es 0 o 1 usando los parámetros de regresión logística aprendidos (w, b) \n",
        "    \n",
        "    Argumentos:\n",
        "    w -- weights o pesos, un array numpy de tamaño (número atributos, 1)\n",
        "    b -- bias o sesgo, un escalar\n",
        "    X -- datos de tamño (número atributos, número de ejemplos)\n",
        "    \n",
        "    Returna:\n",
        "    Y_prediction -- un array numpy (vector) conteniendo todas las predicciones (0/1) para los ejemplos en X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Computa el vector A prediciendo las probabilidades de que esté divorciado en un evento\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convierte las probabilidades A[0, i] a las predicciones actuales p[0, i]\n",
        "        if A[0][i] <= 0.5:\n",
        "            Y_prediction[0][i] = 0\n",
        "        else:\n",
        "            Y_prediction[0][i] = 1\n",
        "    \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV9S4eWTfDaq",
        "colab_type": "text"
      },
      "source": [
        "Aquíí ya tenemos el modelo completo, aquí solamente se hace todo el proceso de una manera ordenada, ya toda la funcionalidad quedó explicada anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtAh03h1VJ1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCIÓN: modelo\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Construye el modelo de regresión logística llamando las funciones que implementamos anteriormente\n",
        "    \n",
        "    Argumentos:\n",
        "    X_train -- el conjunto de entrenamiento representado por un array numpy de tamaño (número atributos, m_train)\n",
        "    Y_train -- las etiquetas de entrenamiento representadas por un array numpy (vector) de tamaño (1, m_train)\n",
        "    X_test -- conjunto de prueba representadas por un array numpy de tamaño (número atributos, m_test)\n",
        "    Y_test -- etiquetas de la prueba representadas por un array numpy (vector)de tamaño (1, m_test)\n",
        "    num_iterations -- hiperparámetro representando los números de iteraciones para optimizar los parámetros\n",
        "    learning_rate -- hiperparámetro representando la tasa de aprendizaje usado en la regla de actualizacióón de optimize()\n",
        "    print_cost -- puesto en true imprime los costos cada 100 iteraciones\n",
        "    \n",
        "    Returna:\n",
        "    d -- dictionario conteniendo información sobre el modelo.\n",
        "    \"\"\"\n",
        "\n",
        "    # incializa los parámetros con ceros    \n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradiente descendiente \n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Recupera los parámetros w y b desde el dictionario \"parametros\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predice los conjuntos prueba/entrenamiento \n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train) \n",
        "\n",
        "    # imprime los errores de entrenamiento/prueba\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVtNy-GaW1KC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "a250e2db-cf49-401f-f4de-36fed174814e"
      },
      "source": [
        "d = model(X_train, y_train, X_test, y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.487686\n",
            "Cost after iteration 200: 0.429457\n",
            "Cost after iteration 300: 0.385365\n",
            "Cost after iteration 400: 0.349116\n",
            "Cost after iteration 500: 0.318874\n",
            "Cost after iteration 600: 0.293419\n",
            "Cost after iteration 700: 0.271812\n",
            "Cost after iteration 800: 0.253317\n",
            "Cost after iteration 900: 0.237360\n",
            "Cost after iteration 1000: 0.223487\n",
            "Cost after iteration 1100: 0.211339\n",
            "Cost after iteration 1200: 0.200630\n",
            "Cost after iteration 1300: 0.191131\n",
            "Cost after iteration 1400: 0.182657\n",
            "Cost after iteration 1500: 0.175057\n",
            "Cost after iteration 1600: 0.168208\n",
            "Cost after iteration 1700: 0.162007\n",
            "Cost after iteration 1800: 0.156368\n",
            "Cost after iteration 1900: 0.151221\n",
            "train accuracy: 97.77777777777777 %\n",
            "test accuracy: 97.14285714285714 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDbM6dY1LD0y",
        "colab_type": "text"
      },
      "source": [
        "Graficamos la curva de aprendizaje para poder observar como la funcióón costo va disminuyendo, efectivamente, como lo deseamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeyPpAF6c3pV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "f9fe034f-452c-4986-9c82-cf4bb5ab1af9"
      },
      "source": [
        "# Graficamos la curva de apredizaje\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.title(\"Tasa de aprendizaje = \" + str(d['learning_rate']))\n",
        "plt.xlabel(\"iteraciones (por cientos)\")\n",
        "plt.ylabel(\"Costo\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Costo')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy-lZhZLLSS2",
        "colab_type": "text"
      },
      "source": [
        "#Conclusiones\n",
        "La regresión logística por medio de un perceptrón es sumamente poderosa y rápida de calcular computacionalmente hablando, con uso de la vectorización y la eliminación de todo ciclo explicito es la clave del éxito computacional, fácil de entender, explicar y calcular.\n",
        "\n",
        "Desventajas, pese a ser un modelo muy simple, cumple su trabajo, desgraciadamente al ser un modelo lineal con transformación sigmoide no es capaz de generar una clasificación correcta en clases mayormente entrelazadas, es por que este solamente es una introducción al gran tema de las redesneuronales y por ende al aprendizaje profundo."
      ]
    }
  ]
}